<HTML>
<HEAD>
<TITLE>Modeling Prompt Adherence in Student Essays</TITLE>
</HEAD>
<BODY>

<h3>Modeling Prompt Adherence in Student Essays</h3>

<p class=MsoNormal>Isaac Persing and Vincent Ng. <br>
<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pp. 1534-1543, 2014. </p>


<p>Click here for the <a href="acl14-essay.ps">PostScript</a> or <a href="acl14-essay.pdf">PDF</a>
version.
The talk slides are available <a href="../talks.html#acl14-essay">here</a>. </P>



<h3>Abstract</h3>

Recently, researchers have begun exploring methods of scoring student essays with
respect to particular dimensions of quality such as coherence, technical errors,
and prompt adherence.  The work 
on modeling prompt adherence,
however, has been focused mainly 
on whether individual
sentences adhere to the prompt.  We
present a new annotated corpus of essay-level
prompt adherence scores and propose a feature-rich 
approach to scoring essays along the prompt
adherence dimension. Our approach 
significantly
outperforms a knowledge-lean baseline prompt adherence 
scoring system yielding
improvements of up to 16.6%.


<P>

<H3>Dataset</H3> 
The human annotation used in this paper is available from 
<a href="http://www.hlt.utdallas.edu/~persingq/ICLE/">this</a> page.

<P>


<h3>BibTeX entry</h3>

<pre>
@InProceedings{Persing+Ng:14a,
  author = {Isaac Persing and Vincent Ng},
  title = {Modeling Prompt Adherence in Student Essays},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {1534--1543},
  year = 2014}
</pre>


<p align="center"><img src="acl14-rubric.jpg" alt="poster" width="35%";></p>

<HR>
</body>

</html>
