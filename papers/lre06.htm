<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
      <title>10.1007/s10579-007-9031-y</title>
      <link href="/config/css/A++_common.css" type="text/css" rel="styleSheet">
      <link href="/config/css/content_A++_SpringerLink.css" type="text/css" rel="styleSheet">
   </head>
   <body class="for-print">
      <table border="1" cellpadding="2px">
         <tbody>
            <tr class="header">
               <td rowspan="1" colspan="1">Language Resources and Evaluation</td>
            </tr>
            <tr>
               <td rowspan="1" colspan="1">©&nbsp;Springer Science+Business Media B.V.&nbsp;2007</td>
            </tr>
            <tr>
               <td rowspan="1" colspan="1">10.1007/s10579-007-9031-y</td>
            </tr>
         </tbody>
      </table>
      <!--Begin Abstract-->
      <div class="Heading1"><a name="title"></a>Unsupervised morphological parsing of Bengali
      </div>
      <p class="AuthorGroup">Sajib&nbsp;Dasgupta<sup>1&nbsp;<a href="#ContactOfAuthor1"><img alt="Contact Information" src="contact.gif" border="0"></a></sup> and Vincent&nbsp;Ng<sup>1&nbsp;<a href="#ContactOfAuthor2"><img alt="Contact Information" src="contact.gif" border="0"></a></sup></p>
      <table>
         <tbody>
            <tr valign="top">
               <td><span class="Affiliation"><a name="Aff1"></a>(1)&nbsp;</span></td>
               <td><span class="Affiliation">Human Language Technology Research Institute, University of Texas at Dallas, Richardson, TX&nbsp;75083, USA</span></td>
            </tr>
         </tbody>
      </table>
      <p><a name="ContactOfAuthor1"></a></p>
      <table class="Contact">
         <tbody>
            <tr>
               <td valign="top"><img alt="Contact Information" src="contact.gif" border="0"></td>
               <td><strong>Sajib&nbsp;</strong><strong>Dasgupta</strong><strong>&nbsp;(Corresponding author)</strong><br><strong>Email: </strong><a href="mailto:sajib@hlt.utdallas.edu">sajib@hlt.utdallas.edu</a></td>
            </tr>
         </tbody>
      </table>
      <p><a name="ContactOfAuthor2"></a></p>
      <table class="Contact">
         <tbody>
            <tr>
               <td valign="top"><img alt="Contact Information" src="contact.gif" border="0"></td>
               <td><strong>Vincent&nbsp;</strong><strong>Ng</strong><strong></strong><br><strong>Email: </strong><a href="mailto:vince@hlt.utdallas.edu">vince@hlt.utdallas.edu</a></td>
            </tr>
         </tbody>
      </table>
      <p class="Affiliation"><strong>Received: </strong>26&nbsp;August&nbsp;2006&nbsp;&nbsp;<strong>Accepted: </strong>6&nbsp;June&nbsp;2007&nbsp;&nbsp;<strong>Published online: </strong>23&nbsp;August&nbsp;2007
      </p>
      <div class="Abstract"><a name="Abs1"></a><span class="AbstractHeading">Abstract&nbsp;&nbsp;</span>Unsupervised morphological analysis is the task of segmenting words into prefixes, suffixes and stems without prior knowledge
         of language-specific morphotactics and morpho-phonological rules. This paper introduces a simple, yet highly effective algorithm
         for unsupervised morphological learning for Bengali, an Indo&#8211;Aryan language that is highly inflectional in nature. When evaluated
         on a set of 4,110 human-segmented Bengali words, our algorithm achieves an F-score of 83%, substantially outperforming Linguistica,
         one of the most widely-used unsupervised morphological parsers, by about 23%.
      </div>
      <p class="Keyword"><span class="KeywordHeading">Keywords&nbsp;&nbsp;</span>Morphological parsing&nbsp;-&nbsp;Word segmentation&nbsp;-&nbsp;Data annotation&nbsp;-&nbsp;Unsupervised learning&nbsp;-&nbsp;Asian language processing&nbsp;-&nbsp;Bengali
      </p>
      <div class=""><a name="Sec1"></a><hr>
         <div class="heading2"><strong>1 &nbsp;
               				</strong>Introduction
         </div>
         <p class="">While research in Asian language processing has gained a lot of momentum in the past decade, much of this research effort
            has indeed been focusing on only a handful of <i>oriental</i> languages such as Chinese, Korean, and Japanese. On the other hand, being spoken by more than 200 million people residing
            mostly in Bangladesh and the Indian state of West Bengal, Bengali is far less computerized than any of these oriental languages.
            However, with the rapid increase in the amount of Bengali data available in electronic form, there is a practical need for
            developing automatic tools for processing Bengali.
         </p>
         <p class="">Bengali, a member of the Indo&#8211;Aryan language family, has several linguistic characteristics that can potentially complicate
            its automatic processing. First, the Bengali morphology is very productive, especially for verbs, with each root verb taking
            more than 50 different forms. In addition, the Bengali lexicon contains a large number of compound words, i.e., words that
            have more than one root, which can be created from almost any combination of nouns, pronouns and adjectives. The large vocabulary
            as a result of its morphological richness makes it difficult to manually construct a Bengali lexicon. Second, Bengali is more
            or less free word order (even though subject&#8211;object&#8211;verb is the typical word order), thus making its syntactic analysis potentially
            more difficult than that for fixed order languages such as English. Finally, the fact that all Bengali letters have only one
            case complicates the detection of proper nouns in Bengali than in languages with both upper and lower case letters.
         </p>
         <p class="">This paper addresses a fundamental problem in Bengali language processing: <i>morphological parsing</i> (also known as <i>word segmentation</i>). The goal of morphological parsing is to segment a given word into the smallest meaning-bearing elements known as <i>morphemes</i>. For instance, the English word &#8220;unforgettable&#8221; can be divided into three morphemes: &#8220;un&#8221;, &#8220;forget&#8221;, and &#8220;able&#8221;, whereas
            the Bengali word &#8220;&#2437;&#2472;&#2494;&#2471;&#2497;&#2472;&#2495;&#2453;&#2468;&#2494;&#2480;&#8221; (anAdhUnIkTAr)
            <a href="#Fn1"><sup>1</sup></a> can be divided into &#8220;an&#8221; (Prefix), &#8220;AdhUnIk&#8221; (Root), &#8220;TA&#8221; (Suffix), and &#8220;r&#8221; (Inflection). While computational morphology
            has been extensively studied for many European languages, this has not been the case for Bengali.
         </p>
         <p class="">Our goal in this paper is to investigate an <i>unsupervised</i> approach to Bengali morphological parsing, which, to our knowledge, represents the first attempt at applying unsupervised
            learning to this Bengali language processing problem. Unsupervised morphological parsing is typically composed of two steps:
            (1) a <b>morpheme induction</b> step in which morphemes are first automatically acquired from a vocabulary consisting of words taken from a large, unannotated
            corpus, and (2) a <b>segmentation</b> step in which a given word is segmented based on these induced morphemes. The biggest challenge in unsupervised morphological
            parsing, then, lies in the ability to induce morphemes correctly without prior knowledge of language-specific morphotactics
            and morpho-phonological rules. It is worth noticing, though, that unsupervised morphological parsing has achieved considerable
            success for many European languages (e.g., Goldsmith <cite><a href="#CR14">2001</a></cite>; Schone and Jurafsky <cite><a href="#CR18">2001</a></cite>; Creutz <cite><a href="#CR6">2003</a></cite>; Freitag <cite><a href="#CR12">2005</a></cite>; Cavar et al. <cite><a href="#CR4">2006</a></cite>). For instance, Schone and Jurafsky report F-scores of 88%, 92%, and 86% on English, German, and Dutch word segmentation,
            respectively. Nevertheless, empirical evaluations in the recent PASCAL Challenge, <i>Unsupervised Segmentation of Words into Morphemes</i>,
            <a href="#Fn2"><sup>2</sup></a> reveal that the success of unsupervised word segmentation algorithms does not carry over to agglutinative languages such
            as Finnish and Turkish,
            <a href="#Fn3"><sup>3</sup></a> both of which have presented significant challenges to word segmentation researchers because of their morphological richness.
            Being highly inflectional in nature, Bengali is expected to offer similar challenges to researchers as Finnish and Turkish.
         </p>
         <p class="">Not only is Bengali morphological parsing a challenging research problem, its solution is of practical significance. As Pushpak
            Bhattacharyya argues in the COLING/ACL 2006 Asian Language Processing panel discussion, the availability of an accurate word
            segmentation algorithm for morphologically rich languages could substantially reduce the amount of annotated data needed to
            construct practical language processing tools such as part-of-speech taggers for these languages. Since Bengali, like the
            majority of Indo&#8211;Aryan languages, is morphologically rich and yet resource-scarce, Bhattacharyya&#8217;s observation suggests that
            our progress in Bengali morphological parsing can potentially accelerate the development of automatic tools for analyzing
            Bengali and other Indo&#8211;Aryan languages in the absence of large annotated corpora.
         </p>
         <p class="">The major contribution of this paper is the introduction of a morphological parser for Bengali. Specifically, our parser extends
            Keshava and Pitler&#8217;s (<cite><a href="#CR17">2006</a></cite>) algorithm,
            <a href="#Fn4"><sup>4</sup></a> the best performer for English in the aforementioned PASCAL Challenge, with three new techniques (see Sects. <a href="#Sec11">4</a>&#8211;<a href="#Sec12">6</a>) that focus on improving the segmentation of regular words.
            <a href="#Fn5"><sup>5</sup></a> The key features of our algorithm are:
         </p>
         <p class="">
            <i>The algorithm is totally unsupervised:</i> As mentioned above, there have been very few attempts at tackling the Bengali morphological parsing problem (e.g., Chaudhuri
            et&nbsp;al. <cite><a href="#CR5">1997</a></cite>; Bhattacharya et&nbsp;al. <cite><a href="#CR1">2005</a></cite>; Dasgupta and Khan <cite><a href="#CR8">2004</a></cite>; Dash <cite><a href="#CR10">2006</a></cite>), all of which have adopted <i>knowledge-based</i> approaches. These approaches operate by segmenting a word using manually-designed heuristics, which require a lot of linguistic
            expertise and are also time-consuming to construct. Worse still, these heuristics are typically language-specific, implying
            that a new set of heuristics has to be designed for each new language encountered. On the other hand, our algorithm is unsupervised,
            relying solely on <i>language-independent</i> techniques for morpheme induction. To our knowledge, we are the first to apply unsupervised learning to morphological parsing
            of an Indo&#8211;Aryan language.
         </p>
         <p class="">
            <i>The algorithm can segment words with multiple roots:</i> Many existing segmentation algorithms can only be applied to words with one root and one suffix (e.g., DéJean <cite><a href="#CR11">1998</a></cite>; Snover and Brent <cite><a href="#CR20">2001</a></cite>). Goldsmith (<cite><a href="#CR14">2001</a></cite>) relaxes this severe limitation by allowing words with multiple affixes to be segmented correctly. Creutz (<cite><a href="#CR6">2003</a></cite>) moves one step further by enabling the segmentation of words with multiple roots, thus facilitating morphological parsing
            of agglutinative languages. Our algorithm, like Creutz&#8217;s, is capable of segmenting words with multiple prefixes, suffixes
            and roots, as a Bengali word can be composed of a lengthy sequence of alternating roots and affixes.
         </p>
         <p class="">
            <i>The algorithm identifies inappropriate morpheme attachments</i>: Many existing morphological parsers erroneously segment &#8220;ally&#8221; as &#8220;all&nbsp;+&nbsp;y&#8221;, because they fail to identify that the morpheme
            &#8220;y&#8221; should <i>not</i> attach to the word &#8220;all&#8221;. Schone and Jurafsky (<cite><a href="#CR18">2001</a></cite>) represents one of the very few attempts at addressing this <i>inappropriate morpheme attachment</i> problem. Specifically, they introduce a method that exploits the semantic relatedness between word pairs to judge whether
            the attachment of a morpheme to a root is valid, and show that identifying inappropriate attachments can substantially improve
            performance. On the other hand, we propose in this paper a novel use of relative frequency distribution to solve the attachment
            problem. Whereas Schone and Jurafsky&#8217;s method relies on complex co-occurrence statistics for calculating semantic relatedness,
            our system, which just uses word frequency, is shown to be effective in improving segmentation performance and is arguably
            much simpler.
         </p>
         <p class="">When evaluated on a set of 4,110 hand-segmented Bengali words chosen randomly from a news corpus, our segmentation algorithm
            achieves an F-score of 83%, substantially outperforming Linguistica (Goldsmith <cite><a href="#CR14">2001</a></cite>), one of the most widely-used unsupervised morphological parsers, by about 23% in F-score. Unlike ours, none of the existing
            Bengali morphological parsers has been evaluated empirically, presumably due to the lack of annotated datasets. In fact, the
            lack of annotated datasets has been a major obstacle to the computerization of resource-scarce languages such as Bengali.
            Hence, we believe that our dataset would be a valuable addition to the list of resources publicly available for Bengali language
            processing,
            <a href="#Fn6"><sup>6</sup></a> facilitating comparative evaluation of different Bengali word segmentation algorithms.
         </p>
         <p class="">The rest of this paper is organized as follows. Section <a href="#Sec5">2</a> presents related work on unsupervised morphological parsing. In Sect. <a href="#Sec6">3</a>, we describe our basic algorithm for inducing morphemes from our Bengali vocabulary. Sections <a href="#Sec11">4</a>&#8211;<a href="#Sec12">6</a> present three extensions to this basic morpheme induction algorithm. In Sect. <a href="#Sec15">7</a>, we describe our algorithm for segmenting a word in the test set using the automatically acquired morphemes. We then evaluate
            the efficacy of our approach in Sect. <a href="#Sec16">8</a> and conclude with future work in Sect. <a href="#Sec21">9</a>.
         </p>
      </div>
      <div class=""><a name="Sec5"></a><hr>
         <div class="heading2"><strong>2 &nbsp;
               				</strong>Related work
         </div>
         <p class="">As mentioned in the introduction, the problem of unsupervised and minimally supervised morphological learning has been extensively
            studied for English and many other European languages. In this section, we will give an overview of the three major approaches
            to this problem.
         </p>
         <p class="">One common approach to unsupervised morphological learning is to first identify morpheme boundaries and then identify the
            morphemes. For instance, Harris (<cite><a href="#CR16">1955</a></cite>) develops a strategy for identifying morpheme boundaries that checks whether the number of different letters following a
            sequence of letters exceeds some given threshold. Hafer and Weiss (<cite><a href="#CR15">1974</a></cite>) improve Harris&#8217;s algorithm by proposing 15 different heuristics that depend on successor and predecessor frequencies to
            identify morpheme boundaries. Their best heuristic achieves a precision of 0.91 and recall of 0.61 on an English corpus of
            approximately 6,200 word types, which is very small compared to the number of word types typically seen in existing literature
            on unsupervised morphological induction. DéJean (<cite><a href="#CR11">1998</a></cite>) improves Harris&#8217;s segmentation algorithm by first inducing a list of the 100 most frequent morphemes and then using those
            morphemes for word segmentation. The aforementioned PASCAL Challenge on Unsupervised Word Segmentation undoubtedly intensified
            interest in this problem. Among the participating groups, Keshava and Pitler&#8217;s (<cite><a href="#CR17">2006</a></cite>) segmentation algorithm combines the ideas of DéJean and Harris and achieves the best result on the English dataset.
         </p>
         <p class="">Another approach to unsupervised morphological learning is based on an application of the Minimum Description Length (MDL)
            principle. The goal is to find a set of morphemes such that when each word in a given corpus is segmented according to these
            morphemes, the total length of an encoding of the corpus is minimized. Specifically, the Expectation Maximization (EM) algorithm
            is used to iteratively segment a list of words taken from a given corpus using some predefined heuristics until the length
            of the morphological grammar converges to a minimum. Brent et&nbsp;al. (<cite><a href="#CR3">1995</a></cite>) introduce an information-theoretic notion of compression to represent the MDL framework, although the overall aim of their
            work is to find an appropriate set of suffixes from a corpus rather than the correct morphological analysis of each word.
            They use the <i>n</i> most common words in the Wall Street Journal corpus of the Penn Treebank to induce the suffix list, where <i>n</i> ranges from 500 to 8,000. Brent (<cite><a href="#CR2">1999</a></cite>) and Snover and Brent (<cite><a href="#CR20">2001</a></cite>) later propose a Bayesian Model for MDL that yields very few false suffixes over a wide range of input sizes in English and
            French. Goldsmith (<cite><a href="#CR13">1997</a></cite>) tries to find the segmentation point of a word based on the probability and length of the hypothesized stems and affixes.
            In a subsequent paper, Goldsmith (<cite><a href="#CR14">2001</a></cite>) adopts the MDL approach and provides a new information-theoretic compression system that gives a measure of the length of
            the morphological grammar. He applies his algorithm to English and French and reports accuracies of 82.9% and 83.3% respectively.
            He also groups together the possible suffixes for each stem, and introduces the signature paradigm that is helpful for determining
            syntactic word classes (i.e., part-of-speech classes). Motivated by Goldsmith, Creutz (<cite><a href="#CR6">2003</a></cite>) and Creutz and Lagus (<cite><a href="#CR7">2005</a></cite>) propose a probabilistic maximum<i> a posteriori</i> formulation that uses prior distributions of morpheme length and frequency to measure the goodness of an induced morpheme.
            They work on English and Finnish (a highly agglutinative language) and report better accuracy than Goldsmith&#8217;s Linguistica
            morphological parser.
         </p>
         <p class="">The last approach, introduced by Freitag (<cite><a href="#CR12">2005</a></cite>), first automatically clusters the words using local co-occurrence information and then induces the suffixes according to
            the orthographic dissimilarity between the words in different clusters. His segmentation algorithm achieves a high precision
            (0.95) when morphemes are induced from an English vocabulary that consists of the 10&nbsp;K most frequent terms in the Wall Street
            Journal corpus of the Penn Treebank. He also makes the interesting observation that employing a larger vocabulary size (say
            20&nbsp;K) for morpheme induction considerably degrades system precision and recall (0.8 and 0.82, respectively).
         </p>
      </div>
      <div class=""><a name="Sec6"></a><hr>
         <div class="heading2"><strong>3 &nbsp;
               				</strong>The basic morpheme induction algorithm
         </div>
         <p class="">As mentioned in the introduction, our unsupervised morphological parser is composed of two steps: (1) inducing <i>prefixes</i>, <i>suffixes</i> and <i>roots</i> from a vocabulary consisting of words taken from a large, unannotated corpus, and (2) segmenting a word based on these induced
            morphemes. The biggest challenge in unsupervised morphological learning lies in accurately performing step 1 (i.e., morpheme
            induction). This section describes our morpheme induction method.
         </p>
         <div class=""><a name="Sec7"></a><div class="Heading3"><strong>3.1 &nbsp;
                  				</strong>Extracting a list of candidate affixes
            </div>
            <p class="">The first step of our morpheme induction method involves extracting a list of candidate affixes. We rely on a fairly simple
               idea originally proposed by Keshava and Pitler (<cite><a href="#CR17">2006</a></cite>) for extracting candidate prefixes and suffixes. Assume that A and B are two character sequences and AB is the concatenation
               of A and B. If AB and A are both found in the vocabulary, then we extract B as a candidate suffix. Similarly, if AB and B
               are both found in the vocabulary, then we extract A as a candidate prefix. Following previous work (e.g., Goldsmith <cite><a href="#CR14">2001</a></cite>; Schone and Jurafsky <cite><a href="#CR18">2001</a></cite>), we represent the vocabulary using the Trie data structure to allow efficient extraction of affixes.
            </p>
         </div>
         <div class=""><a name="Sec8"></a><div class="Heading3"><strong>3.2 &nbsp;
                  				</strong>Ranking the candidate affixes
            </div>
            <p class="">The above affix induction method is arguably overly simplistic, and therefore can generate many spurious affixes. To exemplify,
               consider the English word pair: &#8220;diverge&#8221; and &#8220;diver&#8221;. From this word pair, our algorithm would induce the candidate suffix
               &#8220;ge&#8221;, which, however, is erroneous. The same problem occurs for Bengali. For example, our algorithm would induce from the
               word pair [&#8220;&#2460;&#2494;&#2482;&#2503;&#2478;&#8221; (JAlEm), &#8220;&#2460;&#2494;&#2482;&#8221; (JAl)] the candidate suffix &#8220;&#2503;&#2478;&#8221; (Em), which again is an erroneous suffix. To address this
               problem, we examine in the rest of this subsection <i>two</i> scoring metrics to score each affix, with the goal of assigning low scores to spurious affixes and subsequently removing
               them from our list of induced affixes.
            </p>
            <p class="">
               <i>Metric 1: Counting the number of word types to which each induced affix attaches</i>. In this metric, we set the score of an affix to be the number of word types to which it attaches in the vocabulary. To understand
               the rationale behind this metric, consider the two suffixes in Bengali: &#8220;&#2503;&#2480;&#8221; (Er) and &#8220;&#2503;&#2478;&#8221; (Em). &#8220;Er&#8221; attaches to 9817 word
               types in our corpus, whereas &#8220;Em&#8221; attaches to only 23. This indicates that &#8220;Er&#8221; is a good affix and &#8220;Em&#8221; is not.
            </p>
            <p class="">
               <i>Metric 2: Incorporating the generative strength</i>. By counting the number of word types to which an affix attaches, metric 1 essentially places the same weight on each word
               when scoring an affix. However, some words are &#8220;better&#8221; than the others for morpheme induction (e.g., words to which many
               different affixes attach), and hence a good word should be given a high weight. Specifically, we assign to each word a weight
               based on its <b>generative strength</b> (i.e., how many distinct induced affixes attach to the word). Given this notion of word strength, in metric 2 we set the
               score of an affix to be the sum of the strengths of the words to which it attaches.
            </p>
            <div class="Para">
               <div class="">To see why it makes sense to assign weights based on word strength, consider the following words in English: &#8220;scholarship&#8221;,
                  &#8220;scholars&#8221;, &#8220;championship&#8221;, &#8220;champions&#8221;. From these words, our basic morpheme induction algorithm will infer that &#8220;hip&#8221; is
                  a suffix. However, if we examine the words to which &#8220;hip&#8221; attaches (e.g., &#8220;scholars&#8221; and &#8220;champions&#8221;), we can see that none
                  of them has generative strength (i.e., no other suffixes attach to these words). Hence, this scoring metric will assign a
                  low score to &#8220;hip&#8221;, which is what we desire. As another example, consider the Bengali words: &#8220;&#2453;&#2482;&#2503;&#2460;&#8221; (klEj), &#8220;&#2453;&#2482;&#2503;&#8221; (klE), &#8220;&#2482;&#2494;&#2455;&#2503;&#2460;&#8221;
                  (lAgEj), &#8220;&#2482;&#2494;&#2455;&#2503;&#8221; (lAgE), &#8220;&#2438;&#2460;&#2495;&#2460;&#8221; (ajIj), &#8220;&#2438;&#2460;&#2495;&#8221;(ajI), 
                  &#8220;
                  <div class="Figure"><a name="Figa"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2494;&#2441;&#2460;&#8221;(hAuj), and 
                  &#8220;
                  <div class="Figure"><a name="Figb"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2494;&#2441;&#8221;(hAu). From these words, our algorithm would induce &#8220;j&#8221; as a candidate suffix. However, since &#8220;klE&#8221;, &#8220;lAgE&#8221;, &#8220;ajI&#8221;, and
                  &#8220;hAu&#8221; lack generative strength, the scoring metric will assign a lower score to the candidate suffix &#8220;j&#8221;, which is again what
                  we desire.
               </div>
            </div>
            <div class="Para">
               <div class="">Neither of the above metrics takes into account an important factor when scoring an induced affix: the <i>length</i> of the affix. As Goldsmith (<cite><a href="#CR14">2001</a></cite>) points out, among the induced affixes, the short ones (especially the single character affixes) are more likely to be spurious
                  than the long ones. This is due to the fact that among different words it is easier to get one character difference at the
                  word boundary than two or three character difference. To address this problem, Goldsmith suggests that a higher weight should
                  be placed on longer affixes. Hence, we modify each of the scoring metrics above by multiplying the score of an affix with
                  the length of the affix. In other words, for the first scoring metric, the score of an affix <i>m</i> is now computed as<a name="Equa"></a><table width="100%">
                     <tbody>
                        <tr>
                           <td align="left">
                              <div><img src="10579_2007_9031_Article_Equa.gif" alt="$$ \hbox {score} (m)=\hbox {length} (m) \times (\hbox{Number of different words to which}\,\,m\,\, \hbox {attaches}) $$" align="middle" vspace="20%" border="0"></div>
                           </td>
                        </tr>
                     </tbody>
                  </table>and for the second scoring metric, the score of an affix <i>m</i> is computed as
                  <a name="Equb"></a><table width="100%">
                     <tbody>
                        <tr>
                           <td align="left">
                              <div><img src="10579_2007_9031_Article_Equb.gif" alt="$$ \hbox {score} (m)=\hbox {length} (m) \times \sum_{w} \hbox {strength} (w) $$" align="middle" vspace="20%" border="0"></div>
                           </td>
                        </tr>
                     </tbody>
                  </table>where <i>w</i> is a word to which <i>m</i> attaches, and strength(<i>w</i>) is the strength of <i>w</i>.
               </div>
            </div>
            <div class="Para">
               <div class="">To investigate which of these two scoring metrics is better, we employ them separately to score the induced affixes. The top-scoring
                  prefixes and suffixes according to metric 1 are shown on the left half of Table&nbsp;<a href="#Tab1">1</a>. All the affixes in both the prefix list and the suffix list are correct, and in fact they represent the most commonly used
                  affixes in Bengali.<a name="Tab1"></a><div class="Capt"><span class="CaptNr">Table&nbsp;1&nbsp;</span>Top N-scoring affixes according to metric 1 (left) and metric 2 (right)
                  </div>
                  <table border="1">
                     <colgroup>
                        <col>
                        <col align="char" char=".">
                        <col>
                        <col align="char" char=".">
                        <col>
                        <col align="char" char=".">
                        <col>
                        <col align="char" char=".">
                     </colgroup>
                     <thead>
                        <tr class="header">
                           <th colspan="4" align="left">
                              <p class="">Top-scoring affixes according to metric 1</p>
                           </th>
                           <th colspan="4" align="left">
                              <p class="">Top-scoring affixes according to metric 2</p>
                           </th>
                        </tr>
                        <tr class="header">
                           <th colspan="2" align="left">
                              <p class="">Prefix list</p>
                           </th>
                           <th colspan="2" align="left">
                              <p class="">Suffix list</p>
                           </th>
                           <th colspan="2" align="left">
                              <p class="">Prefix list</p>
                           </th>
                           <th colspan="2" align="left">
                              <p class="">Suffix list</p>
                           </th>
                        </tr>
                        <tr class="header">
                           <th align="left">
                              <p class="">Prefix</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Score</p>
                           </th>
                           <th align="left">
                              <p class="">Suffix</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Score</p>
                           </th>
                           <th align="left">
                              <p class="">Prefix</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Score</p>
                           </th>
                           <th align="left">
                              <p class="">Suffix</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Score</p>
                           </th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">bi</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">1,054</p>
                           </td>
                           <td align="left">
                              <p class="">Er</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">19,634</p>
                           </td>
                           <td align="left">
                              <p class="">prIkl&#8764;pnA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">23,048</p>
                           </td>
                           <td align="left">
                              <p class="">Er</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">121,936</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">a</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">770</p>
                           </td>
                           <td align="left">
                              <p class="">kE</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">13,456</p>
                           </td>
                           <td align="left">
                              <p class="">kOm&#8764;pAnI</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">20,517</p>
                           </td>
                           <td align="left">
                              <p class="">kE</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">113,584</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">p&#8764;rTI</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">664</p>
                           </td>
                           <td align="left">
                              <p class="">r</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">12,747</p>
                           </td>
                           <td align="left">
                              <p class="">p&#8764;rTIsh&#8764;thAn</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">20,240</p>
                           </td>
                           <td align="left">
                              <p class="">Sh</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">73,184</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">mhA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">651</p>
                           </td>
                           <td align="left">
                              <p class="">O</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">8,213</p>
                           </td>
                           <td align="left">
                              <p class="">nIr&#8764;bAcn</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">20,139</p>
                           </td>
                           <td align="left">
                              <p class="">gUlO</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">65,200</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">p&#8764;r</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">640</p>
                           </td>
                           <td align="left">
                              <p class="">I</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">7,872</p>
                           </td>
                           <td align="left">
                              <p class="">S&#8764;tEdhIyAm</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">20,016</p>
                           </td>
                           <td align="left">
                              <p class="">o</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">56,885</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">SU</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">636</p>
                           </td>
                           <td align="left">
                              <p class="">Sh</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">6,502</p>
                           </td>
                           <td align="left">
                              <p class="">p&#8764;rTIjOgITA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">19,700</p>
                           </td>
                           <td align="left">
                              <p class="">I</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">52,290</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">@</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">626</p>
                           </td>
                           <td align="left">
                              <p class="">E</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">6,218</p>
                           </td>
                           <td align="left">
                              <p class="">p&#8764;rk&#8764;rIyA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">19,635</p>
                           </td>
                           <td align="left">
                              <p class="">gUlOr</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">52,165</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">bIs&#8764;b</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">580</p>
                           </td>
                           <td align="left">
                              <p class="">dEr</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">5,874</p>
                           </td>
                           <td align="left">
                              <p class="">SEn&#8764;cUrI</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">19,481</p>
                           </td>
                           <td align="left">
                              <p class="">E</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">49,459</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">bA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">544</p>
                           </td>
                           <td align="left">
                              <p class="">TE</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">4,296</p>
                           </td>
                           <td align="left">
                              <p class="">anUsh&#8764;thAn</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">18,711</p>
                           </td>
                           <td align="left">
                              <p class="">r</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">48,305</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">sIk&#8764;shA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">500</p>
                           </td>
                           <td align="left">
                              <p class="">gUlO</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">3,440</p>
                           </td>
                           <td align="left">
                              <p class="">Sid&#8764;Dan&#8764;T</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">18,613</p>
                           </td>
                           <td align="left">
                              <p class="">tA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">44,430</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">gN</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">496</p>
                           </td>
                           <td align="left">
                              <p class="">rA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">3,262</p>
                           </td>
                           <td align="left">
                              <p class="">pAr&#8764;tnArsIp</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">18,080</p>
                           </td>
                           <td align="left">
                              <p class="">tI</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">44,208</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">prI</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">486</p>
                           </td>
                           <td align="left">
                              <p class="">tA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">2,592</p>
                           </td>
                           <td align="left">
                              <p class="">SmS&#8764;jA</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">17,700</p>
                           </td>
                           <td align="left">
                              <p class="">dEr</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">43,626</p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  
               </div>
            </div>
            <div class="Para">
               <div class="">Next, we examine the top-scoring prefixes and suffixes according to metric 2 (shown in the right half of Table&nbsp;<a href="#Tab1">1</a>). After incorporating generative strength, we can see that the suffix list does not change much, but surprisingly, all the
                  top-scoring prefixes are spurious. A closer examination of the affix lists also reveals that metric 1 is better scoring metric
                  than metric 2: 78% of the top 50 prefixes induced by metric 1 are correct, whereas the corresponding accuracy for metric 2
                  is only 11%. To investigate the reason, we examined the highest ranking prefix &#8220;&#2474;&#2480;&#2495;&#2453;&#2482;&#2509;&#2474;&#2472;&#2494;&#8221; (prIkl&#8764;pnA) and discovered that
                  many of the words to which &#8220;prIkl&#8764;pnA&#8221; attaches are actually suffixes like &#8220;&#2455;&#2497;&#2482;&#2503;·&#2494;&#8221; (gUlO), &#8220;&#2453;&#2494;&#2480;&#2496;&#8221; (kArII), &#8220;&#2478;&#2468;&#2503;·&#2494;&#8221; (mTO),
                  &#8220;&#2476;&#2495;&#2471;&#8221; (bID) and 
                  &#8220;
                  <div class="Figure"><a name="Figc"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2496;&#2467;&#8221; (hIIN). The problem here is that many suffixes in Bengali are found in the corpus as a complete meaning bearing entity,
                  and so they work as a stem in a prefixed word. As a suffix (working like a stem) generally has a high generative strength,
                  the overall score increases manifold and longer prefixes appear high in the list.
               </div>
            </div>
            <p class="">Hence, we conclude that metric 1 does a better job at scoring candidate affixes than metric 2. Hence, in our basic morpheme
               induction algorithm, we will employ metric 1 to score each affix, and retain an induced affix in our list if and only if its
               score is greater than some pre-defined threshold. Specifically, we employ a threshold of 60 and 40 for prefixes and suffixes,
               respectively. These thresholds are determined based on a small validation set consisting of 500 hand-segmented Bengali words
               that are randomly chosen from our corpus.
               <a href="#Fn7"><sup>7</sup></a>
               
            </p>
         </div>
         <div class=""><a name="Sec9"></a><div class="Heading3"><strong>3.3 &nbsp;
                  				</strong>Extracting a list of candidate roots
            </div>
            <p class="">After filtering the spurious affixes as described in the previous subsection, we extract an <b>initial</b> list of candidate roots using the induced list of affixes as follows. For each word, <i>w</i>, in the vocabulary, we check whether <i>w</i> can be segmented as <i>r</i>&nbsp;+&nbsp;<i>s</i> or <i>p</i>&nbsp;+&nbsp;<i>r</i>, where <i>p</i> is an induced prefix, <i>s</i> is an induced suffix, and <i>r</i> is a word in the vocabulary. If so, then <i>w</i> is <i>not</i> a root and so we do not add it to the root list; otherwise, we add <i>w</i> to the root list. However, since Bengali words can contain multiple roots, it is possible that after stripping off the induced
               affixes from a word, we will end up with a string that is a concatenation of several roots. Hence, we make another pass over
               our initial list of roots to remove those strings that contain multiple roots.
            </p>
         </div>
         <div class=""><a name="Sec10"></a><div class="Heading3"><strong>3.4 &nbsp;
                  				</strong>Extensions to the basic induction algorithm
            </div>
            <p class="">So far, we have described our basic morpheme induction algorithm. For each of the following three sections, we will propose
               an extension to this basic induction algorithm. Specifically, in Sect. <a href="#Sec11">4</a>, we will discuss an extension that involves <i>employing a length-dependent threshold</i>. Sections <a href="#Sec12">5</a> and <a href="#Sec14">6</a> present our second extension (i.e., <i>detecting composite suffixes</i>) and our third extension (i.e., <i>improving root induction</i>), respectively.
            </p>
         </div>
      </div>
      <div class=""><a name="Sec11"></a><hr>
         <div class="heading2"><strong>4 &nbsp;
               				</strong>Employing a length-dependent threshold
         </div>
         <div class="Para">
            <div class="">Let us begin by motivating our first extension, <i>length-dependent threshold</i>. Recall from Sect. <a href="#Sec8">3.2</a> that, in our basic morpheme induction algorithm, we retain an induced morpheme in our list if and only if its score is greater
               than some threshold. However, instead of having the same threshold for all induced morphemes, we will employ a varying threshold
               that depends on the length of a morpheme. In particular, we use larger thresholds for shorter morphemes. The rationale is
               simple: since shorter morphemes (especially those that are of length 1 and 2) are more likely to be erroneous than their longer
               counterparts, it makes more sense to employ larger thresholds for shorter morphemes. We set our length-dependent threshold
               as follows:
               <a name="Equc"></a><table width="100%">
                  <tbody>
                     <tr>
                        <td align="left">
                           <div><img src="10579_2007_9031_Article_Equc.gif" alt="$$  \hbox{Threshold for affix A} = m \times C, $$" align="middle" vspace="20%" border="0"></div>
                        </td>
                     </tr>
                  </tbody>
               </table>where <i>C</i> is a constant set to 40 for suffixes and 60 for prefixes as in Sect. 3.2
               <a name="Equd"></a><table width="100%">
                  <tbody>
                     <tr>
                        <td align="left">
                           <div><img src="10579_2007_9031_Article_Equd.gif" alt="$$ \begin{aligned} \hbox{and }m &amp;= (4 -\hbox{ length(A)) if length(A)} \le 2\\ &amp;= 1 \hbox {if length(A)} &gt; 2 \end{aligned} $$" align="middle" vspace="20%" border="0"></div>
                        </td>
                     </tr>
                  </tbody>
               </table>
               
            </div>
         </div>
         <p class="">We will empirically investigate in Sect. <a href="#Sec16">8</a> whether employing this varying threshold would yield better segmentation performance than employing a length-independent
            threshold.
         </p>
      </div>
      <div class=""><a name="Sec12"></a><hr>
         <div class="heading2"><strong>5 &nbsp;
               				</strong>Detecting composite suffixes
         </div>
         <p class="">Our second extension to the basic morpheme induction algorithm involves the detection of <b>composite suffixes</b>. A composite suffix is a suffix formed by combining multiple suffixes. For instance, &#8220;&#2468;&#2494;&#2453;&#2503;&#8221; (TAkE) is a composite suffix
            that comprises &#8220;&#2468;&#2494;&#8221; (TA) and &#8220;&#2453;&#2503;&#8221; (kE) (like &#8220;ers&#8221; in English which is formed by &#8220;er&#8221; and &#8220;s&#8221;). However, not all suffixes
            formed by combining multiple suffixes are composite. For instance, &#8220;&#2503;&#2480;&#8221; (Er) is a <b>non-composite</b> suffix in Bengali, even though it comprises the two simple suffixes &#8220;&#2503;&#8221;(E) and &#8220;&#2480;&#8221;(r).
         </p>
         <p class="">Our goal is to detect and remove composite suffixes from the list of morphemes induced using our basic algorithm, because
            their presence can produce incorrect segmentation of words. For example, if the composite suffix &#8220;TAkE&#8221; is present in the
            induced morpheme list, then &#8220;&#2477;&#2470;&#2509;&#2480;&#2468;&#2494;&#2453;&#2503;&#8221; (vd&#8764;rTAkE) will be erroneously segmented as &#8220;vd&#8764;r&nbsp;+&nbsp;TAkE&#8221; (note: the correct segmentation
            is &#8220;vd&#8764;r&nbsp;+&nbsp;TA&nbsp;+&nbsp;kE&#8221;). The reason is that the presence of the composite suffix causes the segmentation algorithm to believe
            that &#8220;TAkE&#8221; is a non-divisible unit, leading to under-segmentation.
         </p>
         <p class="">Now the question is: How to detect a composite suffix? Not all strings that can be segmented into two suffixes are actually
            composite suffixes. As we have seen at the beginning of this section, &#8220;Er&#8221;, &#8220;E&#8221; and &#8220;r&#8221; all are valid suffixes but &#8220;Er&#8221; is
            not a composite suffix. Hence, we need a more sophisticated method for detecting composite suffixes. Specifically, our method
            posits a suffix as a composite suffix if both of the following criteria are satisfied.
         </p>
         <div class="Para">
            <div class="">
               <i>Suffix strength</i>: This criterion is motivated by the observation that, given a composite suffix <i>a</i> formed by combining two suffixes <i>a</i>1 and <i>a</i>2, the strength of <i>a</i> (i.e., the number of different words to which <i>a</i> attaches) should be smaller than the minimum of the strength of <i>a</i>1 and the strength of <i>a</i>2. As an example, consider the composite suffix &#8220;fullness&#8221; (&#8220;full&#8221;&nbsp;+&nbsp;&#8220;ness&#8221;) in English. The number of words to which &#8220;full&#8221;
               or &#8220;ness&#8221; attaches is far greater than the number of words to which &#8220;fullness&#8221; attaches in a naturally-occurring corpus. Consider
               the <i>non-composite</i> Bengali suffix &#8220;Er&#8221;. It attaches to 9,817 word types in our corpus, but its component suffix &#8220;E&#8221; only attaches to 6,218 words.
               Hence, this suffix violates the suffix strength criterion and is correctly predicted to be non-composite. However, there are
               suffixes like &#8220;AT&#8221; and &#8220;Ar&#8221; (see the right column of Table&nbsp;<a href="#Tab2">2</a>) that satisfy the suffix strength criterion and yet are not composite. This illustrates why using suffix strength alone is
               not sufficient for determining the compositeness of a suffix.<a name="Tab2"></a><div class="Capt"><span class="CaptNr">Table&nbsp;2&nbsp;</span>Examples of suffixes checked for compositeness
               </div>
               <table border="1">
                  <colgroup>
                     <col>
                     <col>
                     <col align="char" char=".">
                     <col>
                     <col>
                     <col align="char" char=".">
                  </colgroup>
                  <thead>
                     <tr class="header">
                        <th colspan="3" align="left">
                           <p class="">Suffixes determined to be composite</p>
                        </th>
                        <th colspan="3" align="left">
                           <p class="">Suffixes determined to be non-composite</p>
                        </th>
                     </tr>
                     <tr class="header">
                        <th align="left">
                           <p class="">Suffix</p>
                        </th>
                        <th align="left">
                           <p class="">Division</p>
                        </th>
                        <th align="left" char=".">
                           <p class="">Similarity</p>
                        </th>
                        <th align="left">
                           <p class="">Suffix</p>
                        </th>
                        <th align="left">
                           <p class="">Division</p>
                        </th>
                        <th align="left" char=".">
                           <p class="">Similarity</p>
                        </th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">AkE (220)</p>
                        </td>
                        <td align="left">
                           <p class="">A (1,764)&nbsp;+&nbsp;kE (6,728)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.954</p>
                        </td>
                        <td align="left">
                           <p class="">AT (83)</p>
                        </td>
                        <td align="left">
                           <p class="">A (1,764)&nbsp;+&nbsp;T (340)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.45</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">AnO (98)</p>
                        </td>
                        <td align="left">
                           <p class="">A (1,764)&nbsp;+&nbsp;nO (160)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.70</p>
                        </td>
                        <td align="left">
                           <p class="">Ar (854)</p>
                        </td>
                        <td align="left">
                           <p class="">A (1,764)&nbsp;+&nbsp;r (12,747)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.57</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">Ei (1,274)</p>
                        </td>
                        <td align="left">
                           <p class="">E (6,218)&nbsp;+&nbsp;i (7,872)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.96</p>
                        </td>
                        <td align="left">
                           <p class="">IyE (116)</p>
                        </td>
                        <td align="left">
                           <p class="">I (1,246)&nbsp;+&nbsp;yE (325)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.53</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">Eri (445)</p>
                        </td>
                        <td align="left">
                           <p class="">Er (9,817)&nbsp;+&nbsp;i (7,872)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.979</p>
                        </td>
                        <td align="left">
                           <p class="">TA (463)</p>
                        </td>
                        <td align="left">
                           <p class="">T (340)&nbsp;+&nbsp;A (1,764)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.038</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">Tao (82)</p>
                        </td>
                        <td align="left">
                           <p class="">TA (463)&nbsp;+&nbsp;o (8213)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.94</p>
                        </td>
                        <td align="left">
                           <p class="">TE (2,148)</p>
                        </td>
                        <td align="left">
                           <p class="">T (340)&nbsp;+&nbsp;E (6,218)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.057</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">T&#8764;bEr (45)</p>
                        </td>
                        <td align="left">
                           <p class="">T&#8764;b (62)&nbsp;+&nbsp;Er (9817)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.91</p>
                        </td>
                        <td align="left">
                           <p class="">Tm (85)</p>
                        </td>
                        <td align="left">
                           <p class="">T (1,246)&nbsp;+&nbsp;m (236)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.023</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">dEri (107)</p>
                        </td>
                        <td align="left">
                           <p class="">dEr (1,958)&nbsp;+&nbsp;i (7,872)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.95</p>
                        </td>
                        <td align="left">
                           <p class="">Tr (54)</p>
                        </td>
                        <td align="left">
                           <p class="">T (346)&nbsp;+&nbsp;r (12,747)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.07</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">krNE (27)</p>
                        </td>
                        <td align="left">
                           <p class="">krN (84)&nbsp;+&nbsp;E (6218)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.77</p>
                        </td>
                        <td align="left">
                           <p class="">kE (6,728)</p>
                        </td>
                        <td align="left">
                           <p class="">k (332)&nbsp;+&nbsp;E (6,218)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.015</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">CEn (259)</p>
                        </td>
                        <td align="left">
                           <p class="">CE (335)&nbsp;+&nbsp;n (1,478)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.83</p>
                        </td>
                        <td align="left">
                           <p class="">nA (188)</p>
                        </td>
                        <td align="left">
                           <p class="">n (1,478)&nbsp;+&nbsp;A (1,764)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.4</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">ECI (34)</p>
                        </td>
                        <td align="left">
                           <p class="">E (6,218)&nbsp;+&nbsp;CI (144)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.97</p>
                        </td>
                        <td align="left">
                           <p class="">Er (9,817)</p>
                        </td>
                        <td align="left">
                           <p class="">E (6,218)&nbsp;+&nbsp;r (12747)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.43</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">bEn (94)</p>
                        </td>
                        <td align="left">
                           <p class="">bE (147)&nbsp;+&nbsp;n (1,478)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.82</p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>bE (55)</b>
                              
                           </p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>b (156)&nbsp;+&nbsp;E (6218)</b>
                              
                           </p>
                        </td>
                        <td align="char" char=".">
                           <p class="">
                              <b>0.47</b>
                              
                           </p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">lAm (120)</p>
                        </td>
                        <td align="left">
                           <p class="">l (616)&nbsp;+&nbsp;Am (235)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.85</p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>bI (81)</b>
                              
                           </p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>b (156)&nbsp;+&nbsp;I (1246)</b>
                              
                           </p>
                        </td>
                        <td align="char" char=".">
                           <p class="">
                              <b>0.45</b>
                              
                           </p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">lEn (233)</p>
                        </td>
                        <td align="left">
                           <p class="">l (616)&nbsp;+&nbsp;En (597)</p>
                        </td>
                        <td align="char" char=".">
                           <p class="">0.86</p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>c</b>&#8764;<b>CIl (22)</b>
                              
                           </p>
                        </td>
                        <td align="left">
                           <p class="">
                              <b>c</b>&#8764;<b>CI (20)&nbsp;+&nbsp;l (616)</b>
                              
                           </p>
                        </td>
                        <td align="char" char=".">
                           <p class="">
                              <b>0.45</b>
                              
                           </p>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <div class="Capt">
                  <div class="CaptCont">
                     <div class="">The strength of each suffix is parenthesized</div>
                  </div>
                  <div class="CaptCont">
                     <div class="">Composite suffixes that are incorrectly identified as non-composite are boldfaced</div>
                  </div>
               </div>
               
            </div>
         </div>
         <div class="Para">
            <div class="">
               <i>Word-level similarity</i>: This criterion is motivated by the observation that, if a composite suffix (AB) attaches to a word <i>w</i>, then it is highly likely that the first component suffix A will also attach to <i>w</i>. In other words, AB and A should be similar in terms of the words to which they attach. For example, if the composite suffix
               &#8220;ers&#8221; attaches to an English word (e.g., &#8220;sing&#8221;), then its first component suffix &#8220;er&#8221; should attach to the same word. This
               property does not hold for non-composite suffixes, however. For instance, while the non-composite suffix &#8220;ent&#8221; attaches to
               words such as &#8220;absorb&#8221;, its first component suffix &#8220;en&#8221; does not. Given this observation, we can detect composite suffixes
               by first computing the similarity between a suffix (AB) and its first component suffix (A) as follows:
               <a name="Eque"></a><table width="100%">
                  <tbody>
                     <tr>
                        <td align="left">
                           <div><img src="10579_2007_9031_Article_Eque.gif" alt="$$ \hbox {Similarity}(AB,A)=P(A|AB)=\frac{|W^{\prime}|}{|W|} $$" align="middle" vspace="20%" border="0"></div>
                        </td>
                     </tr>
                  </tbody>
               </table>where |W&#8242;| is the number of words to which both AB and A attach, and |W| is the number of words to which AB attaches.
            </div>
         </div>
         <p class="">In other words, the similarity between the two suffixes, AB and A, is the probability of seeing A conditioned on seeing AB.
            If this probability is greater than some threshold (we set it to 0.6) and the first criterion (i.e., suffix strength) is satisfied,
            then we posit AB as a composite suffix. One advantage of the above probabilistic metric is that it can potentially be used
            to select the best segmentation of a word among multiple candidates. For example, &#8220;&#2503;&#2480;&#2439;&#8221; (Eri) is a composite suffix that can
            be segmented as either &#8220;E&nbsp;+&nbsp;ri&#8221; (the incorrect segmentation) or &#8220;Er&nbsp;+&nbsp;i&#8221; (the correct segmentation). Since the similarity
            between &#8220;Eri&#8221; and &#8220;Er&#8221; (0.979) is greater than that between &#8220;Eri&#8221; and &#8220;E&#8221; (0.739), &#8220;Er&nbsp;+&nbsp;i&#8221; is more likely to be the correct
            segmentation of &#8220;Eri&#8221;.
         </p>
         <div class="Para">
            <div class="">Most importantly, composite suffix detection has enabled us to segment many Bengali verbs with complex morphology correctly.
               For example, the actual segmentation of the verb 
               &#8220;
               <div class="Figure"><a name="Figd"></a><img src="MediaObjects/10579_2007_9031_Figd_HTML.gif" alt="MediaObjects/10579_2007_9031_Figd_HTML.gif"></div>&#8221; (hAtCIlAm) is &#8220;hAt&nbsp;+&nbsp;CI&nbsp;+&nbsp;l&nbsp;+&nbsp;Am&#8221;, where &#8220;hAt&#8221; is the root, &#8220;CI&#8221; is the tense (Continuous) marker, &#8220;l&#8221; is the time (Past)
               marker, and &#8220;Am&#8221; is the person (first person) marker. Below we show how our algorithm segments &#8220;hAtCIlAm&#8221; step by step:
               <a name="Equf"></a><table width="100%">
                  <tbody>
                     <tr>
                        <td align="left">
                           <div><img src="10579_2007_9031_Article_Equf.gif" alt="$$ \begin{aligned} \hbox{hAtCIlAm} &amp;= \hbox{hAt} + \hbox{CIlAm}\\ &amp;= \hbox{hAt} + \hbox{CI} + \hbox{lAm \quad [detection of composite suffix CIlAm]}\\ &amp;= \hbox{hAt} + \hbox{CI} + \hbox{l} + \hbox{Am \quad [detection of composite suffix lAm]} \end{aligned} $$" align="middle" vspace="20%" border="0"></div>
                        </td>
                     </tr>
                  </tbody>
               </table>.
            </div>
         </div>
         <p class="">To investigate how reliable suffix strength and word-level similarity are with respect to detecting composite suffixes, we
            (1) apply these two criteria to all the suffixes that are concatenations of multiple suffixes, and (2) determine which are
            composite suffixes and which are not. Results for a randomly selected set of suffixes are shown in Table&nbsp;<a href="#Tab2">2</a>, where the left column lists the suffixes identified by our criteria as composite, and the right column lists the suffixes
            that are identified as non-composite.
         </p>
         <div class="Para">
            <div class="">Note that all the entries in the left column are indeed valid composite suffixes in Bengali. In addition, all but the last
               three entries (&#8220;bE&#8221;, &#8220;bI&#8221; and &#8220;c&#8764;CIl&#8221;, which are different tense markers in Bengali) in the right column are valid non-composite
               suffixes. Failure to detect these three and similar tense markers has resulted in incorrect segmentations of present or past
               continuous and future indefinite forms of Bengali verbs. For example, the word 
               &#8220;
               <div class="Figure"><a name="Fige"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2494;&#2463;&#2476;&#2503;&#8221; (&#8220;hAtbE&#8221;, future tense, third person form of verb &#8220;hAt&#8221;) is under-segmented as &#8220;hAt&nbsp;+&nbsp;bE&#8221; (note: the correct segmentation
               is &#8220;hAt&nbsp;+&nbsp;b&nbsp;+&nbsp;E&#8221;). The reason why the algorithm fails to detect &#8220;bE&#8221; as a composite suffix is that there are not enough words
               in the vocabulary to which the suffix &#8220;b&#8221; (first person, future indefinite tense form of a verb) attaches, and so the similarity
               value between &#8220;bE&#8221; and &#8220;b&#8221; is low (0.47).
            </div>
         </div>
         <p class="">The question, then, is: Why are there not enough words in the vocabulary to which the suffix &#8220;b&#8221; attaches? The reason can
            be attributed to the fact that &#8220;b&#8221; is a first-person marker, but the Bengali corpus from which we extracted our vocabulary
            is composed of news articles, which are normally written in &#8220;Third Person&#8221; form. Unless we have a text collection with different
            verb forms (first, second and third person variations), it would be very difficult to segment Bengali verbs correctly.
         </p>
      </div>
      <div class=""><a name="Sec14"></a><hr>
         <div class="heading2"><strong>6 &nbsp;
               				</strong>Improving root induction
         </div>
         <p class="">Our third extension to the basic morpheme induction algorithm involves improving the root induction method described in Sect.
            <a href="#Sec9">3.3</a>. One potential problem with this root induction method is <i>low recall</i>: many words in the vocabulary that are roots are not present in our induced root list. To see the reason, consider again
            the induction method applied to the English word &#8220;candidate&#8221;. Assuming, without loss of generality, that &#8220;candidate&#8221; and &#8220;candid&#8221;
            are found in the vocabulary and &#8220;ate&#8221; is an induced suffix, the root induction method will incorrectly segment &#8220;candidate&#8221;
            as &#8220;candid&nbsp;+&nbsp;ate&#8221;; as a result, it does not consider &#8220;candidate&#8221; as a root. So, to improve the root induction method, we should
            prevent the segmentation of words like &#8220;candidate&#8221;. One way to do this is to determine that the attachment of the suffix &#8220;ate&#8221;
            to the root &#8220;candid&#8221; to form &#8220;candidate&#8221; is incorrect.
         </p>
         <p class="">Now, the question is: How can we determine whether morpheme attachment (e.g., &#8220;ate&#8221;) relative to a particular root word (e.g.,
            &#8220;candid&#8221;) is correct or not? In this section, we propose a simple yet novel idea of using relative corpus frequency to decide
            whether morpheme attachment to a particular root word is plausible or not. Our idea is based on the following hypothesis:
            if a word, A, is a morphological inflection or derivation of a word, B (i.e., A is formed by attaching an affix <i>m</i> to B), then the frequency of A is likely to be less than that of B. In other words, we hypothesize that the inflectional
            or derivational form of a root word occurs less frequently in the corpus than the root word itself.
            <a href="#Fn8"><sup>8</sup></a>
            
         </p>
         <div class="Para">
            <div class="">To obtain empirical support for our hypothesis, we show in Table&nbsp;<a href="#Tab3">3</a> some randomly chosen Bengali words with their <b>word-root frequency ratios (WRFR)</b>, each of which is obtained by dividing the frequency of a word by the frequency of its root. The word-root pairs in the left
               side of the table are examples of correct attachments, whereas those in the right side are not. Consider the word &#8220;&#2472;&#2494;&#2480;&#2496;&#8221; (nArII)
               in the right side of the table; the WRFR of &#8220;nArII&#8221; and &#8220;nAr&#8221; is 556, which means the corpus frequency of &#8220;nArII&#8221; (1670) is
               far bigger than that of the constituent stem &#8220;nAr&#8221; (3). Hence, our hypothesis correctly predicts that the suffix &#8220;&#2496;&#8221; (II)
               cannot attach to &#8220;nAr&#8221; to form &#8220;nArII&#8221;. Note that WRFR is less than 1 for all the words in the left side of the table, whereas
               it is greater than 1 for all the words in the right side of Table&nbsp;<a href="#Tab3">3</a>.<a name="Tab3"></a><div class="Capt"><span class="CaptNr">Table&nbsp;3&nbsp;</span>Some word-root frequency ratios (WRFRs)
               </div>
               <table border="1">
                  <colgroup>
                     <col>
                     <col>
                     <col>
                     <col>
                     <col>
                     <col>
                  </colgroup>
                  <thead>
                     <tr class="header">
                        <th colspan="3" align="left">
                           <p class="">Examples of correct attachments</p>
                        </th>
                        <th colspan="3" align="left">
                           <p class="">Examples of incorrect attachments</p>
                        </th>
                     </tr>
                     <tr class="header">
                        <th align="left">
                           <p class="">Word</p>
                        </th>
                        <th align="left">
                           <p class="">Root</p>
                        </th>
                        <th align="left">
                           <p class="">WRFR</p>
                        </th>
                        <th align="left">
                           <p class="">Word</p>
                        </th>
                        <th align="left">
                           <p class="">Root</p>
                        </th>
                        <th align="left">
                           <p class="">WRFR</p>
                        </th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">@SrEr (&#2438;&#2488;&#2480;&#2503;&#2480;)</p>
                        </td>
                        <td align="left">
                           <p class="">@Sr</p>
                        </td>
                        <td align="left">
                           <p class="">34/200 =&nbsp; 0.17</p>
                        </td>
                        <td align="left">
                           <p class="">nArII (&#2472;&#2494;&#2480;&#2496;)</p>
                        </td>
                        <td align="left">
                           <p class="">nAr</p>
                        </td>
                        <td align="left">
                           <p class="">1,670/3 =&nbsp; 556</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">@bEgE (&#2438;&#2476;&#2503;&#2455;&#2503;)</p>
                        </td>
                        <td align="left">
                           <p class="">@bEg</p>
                        </td>
                        <td align="left">
                           <p class="">28/71 =&nbsp; 0.39</p>
                        </td>
                        <td align="left">
                           <p class="">JAbTIy (&#2479;&#2494;&#2476;&#2468;&#2496;&#2527;)</p>
                        </td>
                        <td align="left">
                           <p class="">JAbT</p>
                        </td>
                        <td align="left">
                           <p class="">198/3 =&nbsp; 66</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">jIIbnKE (&#2460;&#2496;&#2476;&#2472;&#2453;&#2503;)</p>
                        </td>
                        <td align="left">
                           <p class="">jIIbn</p>
                        </td>
                        <td align="left">
                           <p class="">63/908&nbsp;= 0.0693</p>
                        </td>
                        <td align="left">
                           <p class="">KOlA (&#2454;&#2503;·&#2494;&#2482;&#2494;)</p>
                        </td>
                        <td align="left">
                           <p class="">KOl</p>
                        </td>
                        <td align="left">
                           <p class="">587/4 =&nbsp; 146.75</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">Apb&#8764;jy (&#2437;&#2474;&#2476;#x09CD;&#2479;&#2527;)</p>
                        </td>
                        <td align="left">
                           <p class="">b&#8764;jy</p>
                        </td>
                        <td align="left">
                           <p class="">8/940 =&nbsp; 0.0085</p>
                        </td>
                        <td align="left">
                           <p class="">jAmAyAT (&#2460;&#2494;&#2478;&#2494;&#2527;&#2494;&#2468;)</p>
                        </td>
                        <td align="left">
                           <p class="">jAmAy</p>
                        </td>
                        <td align="left">
                           <p class="">996/5 =&nbsp; 199.2</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">upjATi (&#2441;&#2474;&#2460;&#2494;&#2468;&#2495;)</p>
                        </td>
                        <td align="left">
                           <p class="">jATi</p>
                        </td>
                        <td align="left">
                           <p class="">17/509 =&nbsp; 0.033</p>
                        </td>
                        <td align="left">
                           <p class="">bAjAr (&#2476;&#2494;&#2460;&#2494;&#2480;)</p>
                        </td>
                        <td align="left">
                           <p class="">bAj</p>
                        </td>
                        <td align="left">
                           <p class="">1,093/3 =&nbsp; 364.3</p>
                        </td>
                     </tr>
                     <tr class="noclass">
                        <td align="left">
                           <p class="">p&#8764;rTIdIn (&#2474;&#2509;&#2480;&#2468;&#2495;&#2470;&#2495;&#2472;)</p>
                        </td>
                        <td align="left">
                           <p class="">dIn</p>
                        </td>
                        <td align="left">
                           <p class="">728/6,932&nbsp;= 0.105</p>
                        </td>
                        <td align="left">
                           <p class="">jbAb (&#2460;&#2476;&#2494;&#2476;)</p>
                        </td>
                        <td align="left">
                           <p class="">jbA</p>
                        </td>
                        <td align="left">
                           <p class="">813/3 =&nbsp; 271</p>
                        </td>
                     </tr>
                  </tbody>
               </table>
               
            </div>
         </div>
         <p class="">The question, then, is: To what extent does our hypothesis hold true? To investigate this question, we selected 400 words
            from our vocabulary that can be segmented as Prefix&nbsp;+&nbsp;Root or Root&nbsp;+&nbsp;Suffix and removed (1) proper nouns and (2) words whose
            constituent root word is absent in the vocabulary thus lacking root frequency information (e.g., &#8220;&#2438;&#2488;&#2476;&#8221;, @sb= &#8220;@s&nbsp;+&nbsp;b&#8221; but
            &#8220;@s&#8221; is not found in the vocabulary). The final list contains 287 words. We then hand-segmented each of these words into Prefix&nbsp;+&nbsp;Root
            or Root&nbsp;+&nbsp;Suffix, and computed the WRFR ratio for each word-root pair. We found that the WRFR is less than one in 83.56% of
            the 257 words. This provides reasonably strong evidence for our hypothesis that during attachment, the frequency of a word
            is less than that of its constituent root word. Among the remaining 16.44% of the words that violate our hypothesis, we found
            that many of them that should be segmented as Root&nbsp;+&nbsp;Suffix are verbal inflections. In Bengali, inflected forms of the verb
            roots occur more often in the corpus than the roots (e.g., &#8220;&#2453;&#2480;&#2503;&#8221; (kre) occurs more often than &#8220;kr&#8221;). This can be attributed
            to the grammatical rule that says that the main verb of a sentence has to be inflected according to the subject in order to
            maintain sentence order.
         </p>
         <p class="">Since we have shown that our hypothesis is correct to a fairly large extent, we can now use relative frequency information
            to identify incorrect morpheme attachments and improve root induction. Specifically, we incorporate relative frequency information
            in our basic root induction method as follows: For each word, <i>w</i>, in our vocabulary, we check (1) whether <i>w</i> can be segmented into any of <i>r</i>&nbsp;+&nbsp;<i>s</i> or <i>p</i>&nbsp;+&nbsp;<i>s</i> pattern, where <i>p</i> and <i>s</i> are valid prefixes and suffixes respectively and <i>r</i> is another word in the vocabulary, and (2) whether WRFR in between <i>w</i> and <i>r</i> is less than some predefined threshold (&gt;1). If <i>w</i> satisfies both constraints, it means that <i>w</i> is segmentizable, and hence we <b>do not add</b>
            <i>w</i> to the list of induced roots. Otherwise, we add <i>w</i> into the list of roots. The WFFR threshold is set differently for prefixes and suffixes. Specifically, we set the threshold
            to be 2 for prefix attachment and 10 for suffix attachment. (Note, however, that the result is not sensitive to small changes
            to these thresholds.) We employ a higher threshold for suffixes than prefixes to account for the fact that inflectional words
            (mainly verbal suffixations) normally occur more frequently than their corresponding root forms.
         </p>
      </div>
      <div class=""><a name="Sec15"></a><hr>
         <div class="heading2"><strong>7 &nbsp;
               				</strong>Word segmentation
         </div>
         <p class="">In Sect. <a href="#Sec6">3</a>&#8211;<a href="#Sec14">6</a>, we described how we induce a good list of affixes and roots. After inducing the morphemes, we can use them to segment a
            word in the test set into a sequence of morphemes, <i>m</i>
            <sub>1</sub>
            <i>m</i>
            <sub>2</sub> ... <i>m</i>
            <sub>
               <i>n</i>
               </sub>, by adopting a <i>generate-and-remove</i> strategy, as described below.
         </p>
         <p class="">Given a word <i>w</i> in the test set, we (1) <b>generate</b> all possible segmentations of <i>w</i> using only the induced affixes and roots, and then (2) apply a sequence of tests to <b>remove</b> candidate segmentations until we are left with only one candidate, which we take to be the final segmentation of <i>w</i>.
         </p>
         <div class="Para">
            <div class="">Our first test involves removing any candidate segmentation <i>m</i>
               <sub>1</sub>
               <i>m</i>
               <sub>2</sub> ... <i>m</i>
               <sub>
                  <i>n</i>
                  </sub> that violates any of the linguistic constraints below: 
               <table class="OrderedList">
                  <tr valign="top">
                     <td>(1)&nbsp;</td>
                     <td>At least one of <i>m</i>
                        <sub>1</sub>, <i>m</i>
                        <sub>2</sub>,..., <i>m</i>
                        <sub>
                           <i>n</i>
                           </sub> is a root.
                     </td>
                  </tr>
                  <tr valign="top">
                     <td>(2)&nbsp;</td>
                     <td>For 1 &#8804; <i>i</i> &lt;&nbsp; <i>n</i>, if <i>m</i>
                        <sub>
                           <i>i</i>
                           </sub> is a prefix, then <i>m</i>
                        <sub>
                           <i>i</i>+1</sub> must be a root or a prefix.
                     </td>
                  </tr>
                  <tr valign="top">
                     <td>(3)&nbsp;</td>
                     <td>For 1&nbsp;&lt; <i>i</i> &#8804; <i>n</i>, if <i>m</i>
                        <sub>
                           <i>i</i>
                           </sub> is a suffix, then the <i>m</i>
                        <sub>
                           <i>i</i>&#8722;1</sub> must be a root or a suffix.
                     </td>
                  </tr>
                  <tr valign="top">
                     <td>(4)&nbsp;</td>
                     <td>
                        <i>m</i>
                        <sub>1</sub> can not be a suffix and <i>m</i>
                        <sub>
                           <i>n</i>
                           </sub> can not be a prefix.
                     </td>
                  </tr>
               </table>
               
            </div>
         </div>
         <p class="">Next, we apply our second test, in which we retain only those candidate segmentations that have the smallest number of morphemes.
            For example, if &#8220;&#2476;&#2494;&#2482;&#2453;&#2455;&#2497;&#2482;&#2503;·&#2492;&#2494;&#8221; (bAlkgUlO) has two candidate segmentations: &#8220;bAlk&nbsp;+&nbsp;gUlO&#8221; and &#8220;bAl&nbsp;+&nbsp;k&nbsp;+&nbsp;gUlO&#8221;, then we select
            the first one to be the segmentation of <i>w</i>.
         </p>
         <div class="Para">
            <div class="">If more than one candidate segmentation still remains, we apply our third test to remove any candidate <i>c</i> that <i>satisfies</i> one of the three cases below.
               
               <table class="OrderedList" border="0">
                  <tbody>
                     <tr valign="top">
                        <td>&nbsp;</td>
                        <td>
                           <i>Case 1</i>: There exists a root <i>r</i> in <i>c</i> such that <i>r</i> is immediately preceded by a prefix <i>p</i> and immediately followed by a suffix <i>s</i>, but neither the substring <i>pr</i> nor the substring <i>rs</i> is in our vocabulary.
                        </td>
                     </tr>
                     <tr valign="top">
                        <td>&nbsp;</td>
                        <td>
                           <i>Case 2</i>: There exists a root <i>r</i> in <i>c</i> such that <i>r</i> is immediately preceded by a prefix <i>p</i> but <i>not</i> immediately followed by a suffix, and the substring <i>pr</i> is <i>not</i> in our vocabulary.
                        </td>
                     </tr>
                     <tr valign="top">
                        <td>&nbsp;</td>
                        <td>
                           <i>Case 3</i>: There exists a root <i>r</i> in <i>c</i> such that <i>r</i> is immediately followed by a suffix <i>s</i> but not immediately preceded by a prefix, and the substring <i>rs</i> is <i>not</i> in our vocabulary.
                        </td>
                     </tr>
                  </tbody>
               </table>
               
            </div>
         </div>
         <p class="">As an example of applying the third test described above, consider segmenting the Bengali word &#8220;&#2438;&#2480;&#2476;&#2495;&#2468;&#2503;&#8221; (@rbITE). This word
            has two candidate segmentations (&#8220;@rb&nbsp;+&nbsp;I&nbsp;+&nbsp;TE&#8221; and &#8220;@rb&nbsp;+&nbsp;IT&nbsp;+&nbsp;E&#8221;), both of which follow the Root&nbsp;+&nbsp;Suffix&nbsp;+&nbsp;Suffix pattern.
            Since &#8220;@rbI&#8221; is in our vocabulary whereas &#8220;@rbIT&#8221; is not, we remove &#8220;@rb&nbsp;+&nbsp;IT&nbsp;+&nbsp;E&#8221; from our list of candidate segmentations
            (because the second case is satisfied) but retain &#8220;@rb&nbsp;+&nbsp;I&nbsp;+&nbsp;TE&#8221; (because none of the three cases is satisfied).
         </p>
         <p class="">If more than one candidate still remains, we score each remaining candidate using the heuristic below, selecting the highest-scoring
            candidate to be the final segmentation of <i>w</i>. Basically, we score each candidate segmentation by summing up the <i>strength</i> of each morpheme in the segmentation, where (1) the strength of a prefix/suffix is simply the number of word types in the
            vocabulary to which the prefix/suffix attaches, multiplied by the length of the prefix/suffix, and (2) the strength of a root
            is the number of distinct morphemes that attach to it, again multiplied by the length of the root. For example, the word &#8220;&#2438;&#2458;&#2480;&#2467;&#2503;&#8221;
            (@crNE) has two segmentation options: &#8220;@crN&nbsp;+&nbsp;E&#8221; and &#8220;@c&nbsp;+&nbsp;rNE&#8221;. The strengths of the morphemes &#8220;@crN&#8221;, &#8220;E&#8221;, &#8220;@c&#8221; and &#8220;rNE&#8221;
            are 80, 5,937, 26 and 33, respectively. So we select &#8220;@crN&nbsp;+&nbsp;E&#8221; as the final segmentation, because it has the highest strength
            (6,017=80&nbsp;+&nbsp;5,937).
         </p>
      </div>
      <div class=""><a name="Sec16"></a><hr>
         <div class="heading2"><strong>8 &nbsp;
               				</strong>Evaluation
         </div>
         <p class="">In this section, we evaluate our morphological parsing algorithm.</p>
         <div class=""><a name="Sec17"></a><div class="Heading3"><strong>8.1 &nbsp;
                  				</strong>Experimental setup
            </div>
            <p class="">
               <i>Vocabulary creation</i>: The corpus from which we extract our vocabulary contains one year of news articles taken from the Bengali newspaper <i>Prothom Alo</i>. Specifically, we only use articles that are sports news or editorials, as well as those that appear in the first page and
               the last page of the newspaper.
               <a href="#Fn9"><sup>9</sup></a> We then pre-process each of these articles by tokenizing it and removing punctuations and other unwanted character sequences
               (such as &#8220;***&#8221;). The remaining words are then used to create our vocabulary, which consists of 142,955 word types. Unlike
               morphological analysis for many European languages, however, we do not take the conventional step of removing proper nouns
               from our vocabulary, because we do not have a name entity identifier for Bengali.
            </p>
            <p class="">
               <i>Test set preparation</i>: To create our test set, we randomly choose 5,000 words from our vocabulary that are at least 3-character long. We impose
               this length restriction when selecting our test cases simply because words of length one or two do not have any morphological
               segmentation in Bengali. We then manually remove the proper nouns and words with spelling mistakes from the test set before
               giving it to two of our linguists for hand-segmentation. In the absence of a complete knowledge-based morphological parsing
               tool and a hand-tagged morphological database for Bengali, our linguists had to depend on two Bengali dictionaries
               <a href="#Fn10"><sup>10</sup></a> for annotating our test cases.
            </p>
            <p class="">There is one caveat in our manual annotation procedure, however. Many Bengali words are morphologically derived from Sanskrit
               roots.
               <a href="#Fn11"><sup>11</sup></a> These words are very difficult, if not impossible, for any morphological analyzer to segment correctly, because the orthographic
               changes that take place during the segmentation process are highly non-linear and complex in nature. One example of such word
               is &#8220;&#2476;&#2495;&#2480;&#2497;&#2470;&#2509;&#2471;&#8221; (bIrUd&#8764;D), whose actual segmentation is &#8220;&#2476;&#2495;+&#2480;&#2497;&#2471;+&#2453;&#2509;&#2468;(&#2468;)&#8221; (bI&nbsp;+&nbsp;rUD&nbsp;+&nbsp;k&#8764;T (T))&#8212;which is tough to obtain. As a result,
               we instruct our linguists to simplify the segmentation of these words so that the orthographic changes are within tractable
               edit distance. Given this restriction, the Bengali word shown above (i.e., &#8220;&#2476;&#2495;&#2480;&#2497;&#2470;&#2509;&#2471;&#8221;) will simply be segmented as &#8220;&#2476;&#2495;+&#2480;&#2497;&#2470;&#2509;&#2471;&#8221;
               (bI&nbsp;+&nbsp;rUd&#8764;D). However, if the meaning derived from the segmented word differs from that of the original word, then we simply
               treat the original word as a root (i.e., the word should not be segmented at all). Words that fall within this category include
               &#8220;&#2474;&#2509;&#2480;&#2471;&#2494;&#2472;&#8221; (p&#8764;rdhAn), &#8220;&#2438;&#2476;&#2503;&#2470;&#2472;&#8221; (@bEdn), and &#8220;&#2474;&#2509;&#2480;&#2468;&#2495;&#2476;&#2503;&#2470;&#2472;&#8221; (p&#8764;rTIbEdn), for instance. After all the words have been manually segmented,
               we remove those for which the two linguists produce inconsistent segmentations. The resulting test set contains 4,110 words.
            </p>
            <div class="Para">
               <div class="">
                  <i>Evaluation metrics</i>: We use two standard metrics&#8212;<i>exact accuracy</i> and <i>F-score</i>&#8212;to evaluate the performance of our morphological parser on the test set. Exact accuracy is the percentage of the words whose
                  proposed segmentation (S<sub>
                     <i>P</i>
                     </sub>) is identical to the correct segmentation (S<sub>
                     <i>c</i>
                     </sub>). F-score is simply the harmonic mean of recall and precision, as computed using the formulas below.
                  <a name="Equg"></a><table width="100%">
                     <tbody>
                        <tr>
                           <td align="left">
                              <div><img src="10579_2007_9031_Article_Equg.gif" alt="$$ \begin{array}{l} \hbox{Precision} = (\hbox{H}) / (\hbox{H}+\hbox{I})\\ \hbox{Recall} = (\hbox{H}) / (\hbox{H}+\hbox{D})\\ \hbox{F-score} = (2\hbox{H}) / (2\hbox{H}+\hbox{I}+\hbox{D}) \end{array} $$" align="middle" vspace="20%" border="0"></div>
                           </td>
                        </tr>
                     </tbody>
                  </table>where H is the number of Hits (i.e., correctly placed boundaries), and I, D represent the number of morpheme boundaries needed
                  to be inserted into and deleted from S<sub>
                     <i>c</i>
                     </sub>, respectively, to make it identical to S<sub>
                     <i>p</i>
                     </sub>. For instance, comparing the incorrect segmentation &#8220;un&nbsp;+&nbsp;fri&nbsp;+&nbsp;endly&#8221; against the correct segmentation &#8220;un&nbsp;+&nbsp;friend&nbsp;+&nbsp;ly&#8221;,
                  we obtain 1 Hit, 1 Insertion and 1 Deletion, thus yielding a F-score of 0.5 and an exact accuracy of 0. Note that most previous
                  work simply reports results in terms of F-score, which is a less stringent evaluation metric than exact accuracy. However,
                  we believe that reporting results in terms of both metrics will give us a better picture of the strengths and weaknesses of
                  a morphological parser.
               </div>
            </div>
         </div>
         <div class=""><a name="Sec21"></a><div class="Heading3"><strong>8.2 &nbsp;
                  				</strong>Results
            </div>
            <div class="Para">
               <div class="">
                  <i>The baseline system</i>: Following Schone and Jurafsky (<cite><a href="#CR18">2001</a></cite>), we use Goldsmith&#8217;s (<cite><a href="#CR14">2001</a></cite>) Linguistica
                  <a href="#Fn12"><sup>12</sup></a> as our baseline system for unsupervised morphological learning. The first row of Table&nbsp;<a href="#Tab4">4</a> shows the results of our baseline system on the test set when it is trained on the Bengali corpus described in Sect. <a href="#Sec17">8.1</a> (with all the training parameters set to their default values). As we can see, the exact accuracy is about 36%. On the other
                  hand, the baseline achieves a decent F-score of 60.63%. This indicates that many of the analyses returned by Linguistica are
                  only partially correct rather than exactly correct. A closer examination of Linguistica&#8217;s output reveals that it is particularly
                  weak at segmenting Bengali compound words and its complex verbal inflectional system.<a name="Tab4"></a><div class="Capt"><span class="CaptNr">Table&nbsp;4&nbsp;</span>Results. The best exact accuracy and F-score are highlighted.
                  </div>
                  <table border="1">
                     <colgroup>
                        <col>
                        <col align="char" char=".">
                        <col align="char" char=".">
                        <col align="char" char=".">
                        <col align="char" char=".">
                     </colgroup>
                     <thead>
                        <tr class="header">
                           <th align="left">
                              <p class="">System variations</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Exact accuracy (%)</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Precision (%)</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">Recall (%)</p>
                           </th>
                           <th align="left" char=".">
                              <p class="">F-score (%)</p>
                           </th>
                        </tr>
                     </thead>
                     <tbody>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">Baseline</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">36.32</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">58.23</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">63.27</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">60.63</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">Basic induction</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">47.05</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">76.14</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">65.15</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">70.22</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">Length dependent thresholds</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">48.95</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">78.37</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">65.47</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">71.34</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">Detecting composite suffixes</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">58.66</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">79.44</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">82.1</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">80.75</p>
                           </td>
                        </tr>
                        <tr class="noclass">
                           <td align="left">
                              <p class="">Improving root induction</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">
                                 <b>64.62</b>
                                 
                              </p>
                           </td>
                           <td align="char" char=".">
                              <p class="">86.64</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">80.02</p>
                           </td>
                           <td align="char" char=".">
                              <p class="">
                                 <b>83.19</b>
                                 
                              </p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  
               </div>
            </div>
            <p class="">
               <i>Our segmentation algorithm</i>: Results of our segmentation algorithm are shown in rows 2&#8211;5 of Table&nbsp;<a href="#Tab4">4</a>. Specifically, row 2 shows the results of our segmentation algorithm when used in conjunction with the basic morpheme induction
               methods described in Sects. <a href="#Sec7">3.1</a>&#8211;<a href="#Sec9">3.3</a>. Rows 3&#8211;5 show the results when our techniques for employing length-dependent thresholds, detecting composite suffixes, and
               improving root induction are incorporated into the basic system one after the other. It is worth mentioning that (1) our basic
               algorithm already outperforms the baseline system by a wide margin in terms of both evaluation metrics; and (2) while each
               of our additions to the basic algorithm boosts system performance, composite suffix detection and improved root induction
               contribute to performance improvements particularly significantly. As we can see, the best segmentation performance is achieved
               when all of our three additions are applied to the basic algorithm.
               <a href="#Fn13"><sup>13</sup></a> We also performed 5-fold cross validation and found that each addition to the system improves performance statistically significantly
               at p = 0.05.
            </p>
         </div>
         <div class=""><a name="Sec22"></a><div class="Heading3"><strong>8.3 &nbsp;
                  				</strong>Discussion and error analysis
            </div>
            <div class="Para">
               <div class="">As part of the analysis of our algorithm, we examine whether our morphological analyzer can handle complicated test cases.
                  We found that our system successfully segments complex verbal inflections like &#8220;&#2470;&#2497;&#2482;&#2495;&#2527;&#2503;
                  
                  <div class="Figure"><a name="Figh"></a><img src="MediaObjects/10579_2007_9031_Figc_HTML.gif" alt="MediaObjects/10579_2007_9031_Figc_HTML.gif"></div>&#2495;&#2482;&#8221; (dUlIyECIl) as &#8220;dUl&nbsp;+&nbsp;IyE&nbsp;+&nbsp;CI&nbsp;+&nbsp;l&#8221;, and multi-root words like &#8220;&#2476;&#2495;&#2467;&#2503;·&#2492;&#2494;&#2470;&#2472;&#2453;&#2503;&#2472;&#2509;&#2470;&#2509;&#2480;&#2455;&#2497;&#2482;&#2503;·&#2492;&#2494;&#2451;&#8221; (bInOdnkEndRgUlOo) as &#8220;bInOd&nbsp;+&nbsp;n&nbsp;+&nbsp;kEndR&nbsp;+&nbsp;gUlO&nbsp;+&nbsp;o&#8221;.
                  Even more interestingly, it correctly parses English words, which are widely used in the Sports section of the newspaper.
                  For example, words like &#8220;&#2476;&#2482;&#2495;&#2434;&#8221; (blIng) and &#8220;&#2475;&#2494;&#2439;&#2472;&#2494;&#2482;&#2495;&#2488;&#2509;&#2463;&#8221;(FAinAlIS&#8764;t) are correctly segmented as &#8220;bl&nbsp;+&nbsp;Ing&#8221; and &#8220;FAinAl&nbsp;+&nbsp;IS&#8764;t&#8221;,
                  respectively. It is worth mentioning that the compounding nature of Bengali and the influence of foreign languages have introduced
                  into our repository a lot of new words, whose presence increases the difficulty of the segmentation task. Nevertheless, our
                  morphological parser manages to stem those words correctly.
               </div>
            </div>
            <p class="">We also examined the words that were incorrectly segmented by our system. The errors can be broadly divided into following
               categories:
            </p>
            <p class="">(1) <i>Verbal inflections</i>: These constitute a large portion of the words incorrectly segmented by our algorithm. There are two reasons for such errors.
               First, the root of an incorrectly segmented verb is missing from the corpus. For instance, &#8220;&#2441;&#2464;&#2494;&#8221; (uthA) is incorrectly segmented
               because its root &#8220;&#2441;&#2464;&#8221; (uth) is not found in the corpus. Second, the first and second person forms of verbs are often missing
               in the corpus, as the newspaper articles from which our vocabulary is induced contain mostly third person forms of verbs.
            </p>
            <div class="Para">
               <div class="">(2) <i>Irregular words</i>: When root words exhibit orthographic spelling changes during attachment, our system fails to identify the roots. For example,
                  &#8220;&#2480;&#2495;&#2453;&#2509;&#2488;&#2494;&#2480;
                  
                  <div class="Figure"><a name="Figi"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2496;&#8221; (rIk&#8764;sArhII) is not correctly segmented, because the root &#8220;&#2438;&#2480;
                  
                  <div class="Figure"><a name="Figj"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2496;&#8221; (@rhII) is changed into &#8220;&#2494;&#2480;
                  <div class="Figure"><a name="Figk"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2496;&#8221; (ArhII) during attachment.
               </div>
            </div>
            <p class="">(3) <i>Incorrect attachments</i>: Although we use relative frequency to detect incorrect morpheme attachments, many incorrect prefixations and suffixations
               remain undetected (e.g., &#8220;&#2486;&#2495;&#2453;&#2482;&#8221; (sIkl) is a root word but it is incorrectly parsed as &#8220;sIk&nbsp;+&nbsp;l&#8221;). This suggests that we need
               a more sophisticated algorithm for incorrect morpheme attachment detection.
            </p>
            <p class="">(4) <i>Unseen roots</i>: Many words remain unsegmented because their constituent root words are absent in the corpus. For example, the root &#8220;&#2472;&#2503;&#2468;&#2499;&#8221;
               (nETR) in &#8220;&#2472;&#2503;&#2468;&#2499;&#2468;&#2509;&#2476;&#8221; (nETRT&#8764;b) is not found in our corpus.
            </p>
         </div>
      </div>
      <div class=""><a name="Sec23"></a><hr>
         <div class="heading2"><strong>9 &nbsp;
               				</strong>Conclusions and future work
         </div>
         <p class="">We have presented a new unsupervised algorithm for Bengali morphological parsing. Our work distinguishes itself from previous
            algorithms for Bengali morphological parsing in two important aspects. First, all previous algorithms adopt knowledge-based
            approaches, thus requiring a lot of time and linguistic expertise to implement. Second, none of them has been empirically
            evaluated, and hence it is unclear how well they perform. Despite its simplicity, our algorithm achieves very promising results:
            when evaluated on a set of 4,110 human-segmented Bengali words, the algorithm achieves an F-score of 83% and an exact accuracy
            of 66%, outperforming Goldsmith&#8217;s Linguistica by 23% in F-score and 28% in exact accuracy. Analysis reveals that our novel
            use of relative frequency information, together with our technique for composite suffix detection, have contributed to the
            superior performance of our algorithm.
         </p>
         <p class="">In future work, we intend to improve our algorithm in a number of ways. First, we will examine the problem of morphologically
            analyzing highly irregular word forms. This involves automatically acquiring transformation rules that specify what characters
            are inserted or deleted during the transformation, and is considered a challenging problem even for morphologically impoverished
            languages such as English (Yarowsky and Wicentowski <cite><a href="#CR21">2000</a></cite>). Second, we plan to employ automatically acquired information about the semantic relatedness between word pairs (see Schone
            and Jurafsky <cite><a href="#CR18">2001</a></cite>) to improve our incorrect attachment detection algorithm. Finally, motivated by Singh et&nbsp;al.&#8217;s (<cite><a href="#CR19">2006</a></cite>) work on Hindi, we plan to investigate how to build a part-of-speech tagger for Bengali that exploits the morphological information
            provided by our algorithm.
         </p>
         <p class="">Bengali language processing is still in its infancy. As mentioned in the introduction, one major obstacle to the computerization
            of Bengali is the scarcity of annotated corpora. As part of our commitment to developing high-performance tools and algorithms
            for automatically analyzing Bengali, we intend to construct annotated datasets for different Bengali language processing problems.
            With annotated data, we hope to advance the state of the art in Bengali language processing by (1) enabling empirical evaluations
            of Bengali language processing systems, and (2) tackling problems in Bengali language processing using corpus-based techniques,
            which are by far the most successful techniques in natural language learning. Above all, we hope to stimulate interest in
            the computerization of Bengali in the natural language processing community.
         </p>
      </div>
      <p></p>
      <hr>
      <h2><a name="Bib1"></a>References
      </h2>
      <table>
         <tbody class="Citation">
            <tr valign="top">
               <td><a name="CR1"></a>Bhattacharya, S., Choudhury, M., Sarkar, S., &amp; Basu, A. (2005). Inflectional morphology synthesis for Bengali noun, pronoun
                  and verb systems. In <i>Proceedings of the national conference on computer processing of Bangla (NCCPB 05)</i>, pp. 34&#8211;43.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR2"></a>Brent, M. R. (1999). An efficient, probabilistically sound algorithm for segmentation and word discovery. <i>Machine Learning, 34</i>, 71&#8211;106.<br>
                  				<a href="http://dx.doi.org/10.1023/A:1007541817488" target="_blank"><img src="springer_link.gif" border="0" alt="SpringerLink" width="108" height="20"></a>
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR3"></a>Brent, M. R., Murthy, S. K., &amp; Lundberg, A. (1995). Discovering morphemic suffixes: A case study in minimum description length
                  induction. In <i>Proceedings of the fifth international workshop on artificial intelligence and statistics</i>.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR4"></a>Cavar, D., Rodriguez, P., &amp; Schrementi, G. (2006). Unsupervised morphology induction for part-of-speech-tagging. In <i>Penn working papers in Linguistics: Proceedings of the 29th annual Penn Linguistics colloquium</i>, Vol. 12.1.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR5"></a>Chaudhuri, B. B., Dash, N. S., &amp; Kundu, P. K. (1997). Computer parsing of Bangla verbs. In <i>Linguistics Today, 1</i>(1), 64&#8211;86.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR6"></a>Creutz, M. (2003). Unsupervised segmentation of words using prior distributions of morph length and frequency. In <i>Proceedings of the 41st annual meeting of the ACL</i>, pp. 280&#8211;287.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR7"></a>Creutz, M., &amp; Lagus, K. (2005). Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor
                  1.0. In Computer and information science, Report A81, Helsinki University of Technology.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR8"></a>Dasgupta, S., &amp; Khan, M. (2004). Feature unification for morphological parsing in Bangla. In <i>Proceedings of international conference on computer and information technology</i>.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR9"></a>Dasgupta, S., &amp; Ng, V. (2007). High-performance, language-independent morphological segmentation. In NAACL-HLT 2007: <i>Proceedings of the main conference</i>, pp. 155&#8211;163.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR10"></a>Dash, N. S. (2006). The Morphodynamics of Bengali Compounds decomposing them for lexical processing. <i>Language in India (www.languageinindia.com), 6</i>, 7.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR11"></a>DéJean, H. (1998). Morphemes as necessary concepts for structures: Discovery from untagged corpora. In <i>Workshop on paradigms and grounding in natural language learning</i>, pp. 295&#8211;299.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR12"></a>Freitag, D. (2005). Morphology induction from term clusters. In <i>Proceedings of the ninth conference on computational natural language learning</i> (CoNLL), pp. 128&#8211;135.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR13"></a>Goldsmith, J. (1997). Unsupervised learning of the morphology of a natural language. University of Chicago. http://humanities.uchicago.edu/faculty/goldsmith.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR14"></a>Goldsmith, J. (2001). Unsupervised learning of the morphology of a natural language. <i>Computational Linguistics, 27</i>(2), 153&#8211;198.<br>
                  				<a href="http://dx.doi.org/10.1162/089120101750300490" target="_blank"><img src="crossref_link.gif" border="0" alt="CrossRef" width="65" height="20"></a>
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR15"></a>Hafer, M. A., &amp; Wess, S. F. (1974). Word segmentation by letter successor varities. <i>Information Storage and Retrieval, 10</i>, 371&#8211;385.<br>
                  				<a href="http://dx.doi.org/10.1016/0020-0271(74)90044-8" target="_blank"><img src="crossref_link.gif" border="0" alt="CrossRef" width="65" height="20"></a>
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR16"></a>Harris, Z. (1955). From phoneme to morpheme. <i>Language, 31</i>(2), 190&#8211;222.<br>
                  				<a href="http://dx.doi.org/10.2307/411036" target="_blank"><img src="crossref_link.gif" border="0" alt="CrossRef" width="65" height="20"></a>
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR17"></a>Keshava, S., &amp; Pitler, E. (2006). A simpler, intuitive approach to morpheme induction. In <i>PASCAL challenge workshop on unsupervised segmentation of words into morphemes</i>.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR18"></a>Schone, P., &amp; Jurafsky, D. (2001). Knowledge-free induction of inflectional morphologies. In <i>Proceedings of the second meeting of the NAACL</i>, pp. 183&#8211;191.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR19"></a>Singh, S., Gupta, K., Shrivastava, M., &amp; Bhattacharyya, P. (2006). Morphological richness offsets resource demand &#8211; experiences
                  in constructing a POS tagger for Hindi. In <i>Proceedings of the COLING/ACL 2006 poster sessions</i>, pp. 779&#8211;786.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR20"></a>Snover, M. G., &amp; Brent, M. R. (2001). A Bayesian model for morpheme and paradigm identification. In <i>Proceedings of the 39th annual meeting of the ACL</i>, pp. 482&#8211;490.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
            <tr valign="top">
               <td><a name="CR21"></a>Yarowsky, D., &amp; Wicentowski, R. (2000). Minimally supervised morphological analysis by multimodal alignment. In <i>Proceedings of the 38th annual meeting of the ACL</i>, pp. 207&#8211;216.
               </td>
            </tr>
            <tr>
               <td>&nbsp;</td>
            </tr>
         </tbody>
      </table>
      <hr>
      <h2>Footnotes</h2>
      <table>
         <tbody class="CaptCont">
            <tr>
               <td valign="top"><a name="Fn1"></a><sup>1</sup></td>
               <td class="">Throughout this paper, we use the Romanized transliteration for Bengali, which is almost phonetic. For example, &#8216;&#2437;&#8217; is &#8216;a&#8217;,
                  &#8216;&#2438;&#8217; is &#8216;@&#8217;, &#8216;&#2494;&#8217; is &#8216;A&#8217;, &#8216;&#2453;&#8217; is &#8216;k&#8217;, &#8216;&#2463;&#8217; is &#8216;t&#8217;, &#8216;&#2468;&#8217; is &#8216;T&#8217;, &#8216;&#2464;&#8217; is &#8216;th&#8217;, etc. We have used &#8216;&#8764;&#8217; for Halant in Bengali. Our
                  transliteration mapping table is shown in our data distribution site at http://www.utdallas.edu/&#8764;sajib/dataset.html
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn2"></a><sup>2</sup></td>
               <td class="">See http://www.cis.hut.fi/morphochallenge2005/</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn3"></a><sup>3</sup></td>
               <td class="">A word in an agglutinative language is composed of a linear sequence of distinct morphemes.</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn4"></a><sup>4</sup></td>
               <td class="">Keshava and Pilter&#8217;s algorithm has been applied to English, Finnish, and Turkish only.</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn5"></a><sup>5</sup></td>
               <td class="">Our morphological parser does not handle the segmentation of words that show orthographic character changes during attachment
                  with other morphemes. Nevertheless, since less than 4% of our test cases correspond to words in this category, not handling
                  them will unlikely lead to a dramatic degradation of system performance.
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn6"></a><sup>6</sup></td>
               <td class="">Our dataset is available at http://www.utdallas.edu/&#8764;sajib/dataset.html</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn7"></a><sup>7</sup></td>
               <td class="">We expect that larger thresholds are needed for languages that have a larger vocabulary (e.g., Turkish and Finnish) because
                  an affix is likely to be generated from a larger number of words.
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn8"></a><sup>8</sup></td>
               <td class="">Note that in many inflectional languages, the root form rarely stands alone, and so the morphologically formed A is likely
                  to be more frequent than its root form. However, from a computational perspective, it is beneficial to exploit this hypothesis
                  in our segmentation algorithm, as it applies to a fairly large percentage of words.
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn9"></a><sup>9</sup></td>
               <td class="">These are the major sections of Prothom Alo. The remaining sections are relatively small and are simply ignored.</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn10"></a><sup>10</sup></td>
               <td class="">The dictionaries are &#8220;&#2476;&#2457;&#2455;&#2496;&#2527; &#2486;&#2476;&#2509;&#2470;&#2453;&#2503; ·&#2494;&#2527;&#8221; (Bangiya Sabdakosh) by &#2489;&#2480;&#2495;&#2458;&#2480;&#2467; &#2476;&#2467;&#2509;&#2470;&#2509;&#2479;&#2503;·&#2494;
                  
                  <div class="Figure"><a name="Figf"></a><img src="MediaObjects/10579_2007_9031_Figb_HTML.gif" alt="MediaObjects/10579_2007_9031_Figb_HTML.gif"></div>&#2494;&#2471;&#2509;&#2527;&#2494;&#2527; (Haricharan Bandopaday) and &#8220;&#2476;&#2494;&#2434;&#2482;&#2494; &#2447;&#2453;&#2494;&#2465;&#2503;&#2478;&#2496; &#2476;&#2509;&#2479;&#2476;
                  <div class="Figure"><a name="Figg"></a><img src="MediaObjects/10579_2007_9031_Figa_HTML.gif" alt="MediaObjects/10579_2007_9031_Figa_HTML.gif"></div>&#2494;&#2480;&#2495;&#2453; &#2476;&#2494;&#2434;&#2482;&#2494; &#2437;&#2477;&#2495;&#2471;&#2494;&#2472;&#8221; (Bangla Academy Bebharic Bangla Avidan).
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn11"></a><sup>11</sup></td>
               <td class="">Sanskrit roots have compact orthography which is not morpho-phonologically transparent. That is, one written unit does not
                  necessarily correspond to one morpheme or syllable.
               </td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn12"></a><sup>12</sup></td>
               <td class="">Linguistica is publicly available at http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/</td>
            </tr>
            <tr>
               <td valign="top"><a name="Fn13"></a><sup>13</sup></td>
               <td class="">It may seem that our performance improvements over Linguistica have come from our fine-tuning the thresholds. However, our
                  system has achieved good performance on English, Turkish and Finnish using almost the same set of thresholds. The only exception
                  is the thresholds used for inducing affixes (see Sect. <a href="#Sec6">3</a>); however, these thresholds can be set automatically depending on the vocabulary size of a language (see Dasgupta and Ng
                  (<cite><a href="#CR9">2007</a></cite>)).
               </td>
            </tr>
         </tbody>
      </table>
   </body>
</html>