<HTML>
<HEAD>
<TITLE>Combining Sample Selection and Error-Driven Pruning for Machine
Learning of Coreference Rules</TITLE>
</HEAD>
<BODY>
<H2>Combining Sample Selection and Error-Driven Pruning for Machine
Learning of Coreference Rules</H2>
Vincent Ng and Claire Cardie. 
<br>
<i>2002 Coreference on Empirical Methods in Natural Language Processing</i> (EMNLP), 2002.
<P>

Click here for the
<a href="emnlp02.ps">PostScript</a> or <a href="emnlp02.pdf">PDF</a>
version. 

<H3>Abstract</H3>
Most machine learning solutions to noun phrase coreference resolution
recast the problem as a classification task. We examine three
potential problems with this reformulation, namely, skewed class
distributions, the inclusion of <i>hard</i> training instances, and
the loss of transitivity inherent in the original coreference
relation. We show how these problems can be handled via intelligent
sample selection and error-driven pruning of classification
rulesets. The resulting system achieves an F-measure of 69.5 and
63.4 on the MUC-6 and MUC-7 coreference resolution data sets,
respectively, surpassing the performance of the best MUC-6 and MUC-7
coreference systems. In particular, the system outperforms the
best-performing learning-based coreference system to date.

<HR> 
</BODY>
</HTML>
