<html>
<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<title>CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models</title>
</head>
<body>

<h3>CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models</h3>
Changan Niu, Chuanyi Li, Vincent Ng, and Bin Luo.
<br>
<i>Proceedings of the 45th IEEE/ACM International Conference on Software Engineering (Research Track)</i>, 2023. 


<p>Click here for the <a href="icse23-cross.pdf">PDF</a> version. 
<!-- The talk slides are available <a href="../talks.html#acl17">here</a>. -->

<p>
<h3>Abstract</h3>

Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value.
In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of ``training/evaluation'' task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.

<p>
<h3>BibTeX entry</h3>

<pre>
@InProceedings{Niu+etal:23b,
  author = {Changan Niu and Chuanyi Li and Vincent Ng and Bin Luo},
  title = {{CrossCodeBench}: Benchmarking Cross-Task Generalization of Source Code Models},
  booktitle = {Proceedings of the 45th IEEE/ACM International Conference on Software Engineering (Research Track)},
  <!-- pages = {5546--5555}, -->
  year = 2023}
</pre>
<br>

<!--
<p align="center"><img src="emnlp13-poster.jpg" alt="poster" width="100%";></p>
-->
<hr>
</body>

</html>
